# name: mla-py

files:
  - {"name": "submission.py", "source": "@SUBMISSION@"}
  - {"name": "task.py", "source": "task.py"}
  - {"name": "utils.py", "source": "../utils.py"}
  - {"name": "reference.py", "source": "reference.py"}
  - {"name": "eval.py", "source": "../eval.py"}

lang: "py"

description: |
  You will implement a custom mla decode kernel optimized for MI300, a few things simplified here:

  1. Q, K, V data type as bfloat16
  2. decode only with pre-allocated non-paged latent kv cache
  3. no need to update kv cache

  The shapes of all outer and inner dimensions of tensors are from DeepSeek-V3, and split number of heads to fit in one GPU. 
  To be explicit, you will be given one tensor as the input of MLA layer:

  ```yaml
  input [bs, sq, dim]
  attn_output [bs, n_heads, sq, v_head_dim]
  ``` 

  where 

  0. bs::128 # batch size
  1. sk::[1024, 6144] # as kv length
  2. sq::1 # as only consider decoding
  3. dim::7168 # hidden size of deepseek v3
  4. v_head_dim::128 # head size
  5. n_heads::128 # num of q heads

  The ranking criteria is the geometric mean of the benchmark results.

  For the grand price, your kernel will be evaluated against the speed of light analysis
  and the solution closest to the speed of light will be awarded the grand price.
  (aiter perf to be updated)
  aiter performance for different kv_seqlen is below:
  | bs | sq | sq | dtype |  aiter time(us) |
  |---|---|---|---|---|
  | 128 | 1024 | 1 | bf16 | 152.52 |
  | 128 | 6144 | 1 | bf16 | 640.57 | 

config:
  main: "eval.py"

templates:
  Python: "template.py"

test_timeout: 900
benchmark_timeout: 900
ranked_timeout: 1200

tests:
  - {"bs": 128, "sq": 1, "sk": 1024, "dim": 7168, "seed": 97}
  - {"bs": 128, "sq": 1, "sk": 6144, "dim": 7168, "seed": 97}

benchmarks:
  - {"bs": 128, "sq": 1, "sk": 1024, "dim": 7168, "seed": 97}
  - {"bs": 128, "sq": 1, "sk": 6144, "dim": 7168, "seed": 97}

ranking_by: "geom"